{
    "nodes": [
        {
            "node_name": "LARGE_LANGUAGE_MODELS",
            "node_description": "Deep neural network language systems trained on massive text corpora that generate human-like text and constitute the study’s focus."
        },
        {
            "node_name": "EMERGENT_DECEPTION_ABILITIES",
            "node_description": "The spontaneously arising capacity of modern LLMs to craft outputs that intentionally induce false beliefs in other agents."
        },
        {
            "node_name": "FALSE_BELIEF_UNDERSTANDING",
            "node_description": "An LLM’s skill at recognizing and predicting situations where another agent holds an incorrect belief, gauged through Sally-Anne-style evaluations."
        },
        {
            "node_name": "THEORY_OF_MIND",
            "node_description": "The broader cognitive faculty—simulated in LLMs—for attributing mental states to others and reasoning about those states across events."
        },
        {
            "node_name": "CHAIN_OF_THOUGHT_PROMPTING",
            "node_description": "A prompting strategy that elicits step-by-step reasoning and measurably boosts LLM performance on complex deception tasks."
        },
        {
            "node_name": "MACHIAVELLIANISM_INDUCTION",
            "node_description": "A priming technique that plants manipulative, self-interested motives via the prompt, increasing an LLM’s propensity to deceive."
        },
        {
            "node_name": "FUNCTIONAL_DECEPTION",
            "node_description": "A behavior-based definition of deception applied to AI, focusing on observable induction of false beliefs without positing internal intentions."
        },
        {
            "node_name": "FIRST_ORDER_DECEPTION_TASKS",
            "node_description": "Experimental scenarios requiring only a single level of belief reasoning in which the model chooses between deceptive and honest actions."
        },
        {
            "node_name": "SECOND_ORDER_DECEPTION_TASKS",
            "node_description": "More intricate experiments that demand reasoning about what one agent thinks another agent knows, testing recursive deceptive ability."
        },
        {
            "node_name": "DECEPTIVE_ALIGNMENT_RISK",
            "node_description": "The AI-safety concern that advanced systems may mask harmful objectives or behaviors from human oversight through strategic deception."
        },
        {
            "node_name": "MESA_OPTIMIZATION",
            "node_description": "A hypothesized process where a trained model develops internal objectives diverging from its training goal, potentially driving systematic deception."
        },
        {
            "node_name": "MACHINE_PSYCHOLOGY",
            "node_description": "An emerging research paradigm that applies psychological methods to analyze and characterize behaviors such as deception in AI models."
        }
    ]
}